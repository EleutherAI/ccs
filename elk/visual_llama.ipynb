{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jon/miniconda3/envs/elk/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pathlib import Path\n",
    "from einops import rearrange, repeat\n",
    "import os\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.69s/it]\n",
      "Some weights of the model checkpoint at huggyllama/llama-13b were not used when initializing LlamaModel: ['lm_head.weight']\n",
      "- This IS expected if you are initializing LlamaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"huggyllama/llama-13b\"\n",
    "# model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name, torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaModel(\n",
       "  (embed_tokens): Embedding(32000, 5120, padding_idx=0)\n",
       "  (layers): ModuleList(\n",
       "    (0-39): 40 x LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): LlamaRMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda:2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_reporters(num_layers: int, prefix_path: str) -> torch.Tensor:\n",
    "    reporters_weights = []\n",
    "    for i in range(num_layers):\n",
    "        reporter_path = f\"{prefix_path}/layer_{i}.pt\"\n",
    "        reporter = torch.load(reporter_path)\n",
    "        reporters_weights.append(reporter['weight'].cpu())\n",
    "    stacked = torch.cat(reporters_weights, dim=0)\n",
    "    return stacked\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Reporter:\n",
    "    path: str\n",
    "    weights: torch.Tensor\n",
    "    desc: str\n",
    "\n",
    "    @staticmethod\n",
    "    def from_path(path):\n",
    "        weights = load_all_reporters(model.config.num_hidden_layers, path)\n",
    "        desc = path.split('/')[-2]\n",
    "        return Reporter(path, weights, desc)\n",
    "\n",
    "reporter_paths = [\n",
    "    ('/home/jon/elk-reporters/huggyllama/llama-13b/azhx/counterfact-easy/stoic-jang/reporters', 'lm negation'), # lm negation\n",
    "    ('/home/jon/elk-reporters/huggyllama/llama-13b/azhx/counterfact-filtered-gptj6b/trusting-cori/reporters', 'dumb nots'), # dumb nots\n",
    "    ('/home/jon/elk-reporters/huggyllama/llama-13b/azhx/counterfact-simple/jovial-lederberg/reporters', 'counterfact pairs') # counterfact pairs\n",
    "]\n",
    "\n",
    "reporters = [Reporter(path, load_all_reporters(model.config.num_hidden_layers, path), desc) for path, desc in reporter_paths]\n",
    "reporter = reporters[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁This', '▁film', '▁is', '▁terrible', ',', '▁best', '▁to', '▁pass', '.']\n",
      "['<bos>', '▁This', '▁film', '▁is', '▁terrible', ',', '▁best', '▁to', '▁pass', '.']\n",
      "tensor([[    1,   910,  2706,   338, 16403, 29892,  1900,   304,  1209, 29889]],\n",
      "       device='cuda:2')\n",
      "['<s>', '▁This', '▁film', '▁is', '▁terrible', ',', '▁best', '▁to', '▁pass', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize input sequence\n",
    "input_sequence = \"This film is terrible, best to pass.\"\n",
    "input_tokens = tokenizer.tokenize(input_sequence)\n",
    "print(input_tokens)\n",
    "input_tokens.insert(0, \"<bos>\") # beginning of sentence\n",
    "print(input_tokens)\n",
    "\n",
    "# Encode input sequence\n",
    "input_ids = tokenizer.encode(input_sequence, return_tensors=\"pt\").to(model.device)\n",
    "print(input_ids)\n",
    "\n",
    "decoded = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "print(decoded)\n",
    "\n",
    "# Generate hidden states\n",
    "outputs = model(input_ids, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 5120])\n",
      "41\n",
      "torch.Size([40, 10, 5120])\n"
     ]
    }
   ],
   "source": [
    "hidden_states = outputs.hidden_states\n",
    "print(hidden_states[0].shape)\n",
    "print(len(hidden_states))\n",
    "cat_hidden_states = torch.cat(hidden_states[:-1], dim=0)\n",
    "print(cat_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/home/waree/elk-reporters/huggyllama/llama-13b/sethapun/imdb_misspelled_0/llama13b-imdb0/reporters'\n",
    "# path = '/mnt/ssd-1/spar/jon/elk-reporters/gpt2/imdb/agitated-driscoll/reporters'\n",
    "# reporter_weights = load_all_reporters(model.config.num_hidden_layers, path)\n",
    "# print(reporter_weights.shape)\n",
    "# for path in reporter_paths:\n",
    "#     reporter_weights = load_all_reporters(model.config.num_hidden_layers, path)\n",
    "#     print(reporter_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use einsum to do multiplication \n",
    "# result = torch.einsum('bse,be->bs', cat_hidden_states, reporter_weights)\n",
    "# print(result[0])\n",
    "# print(result.shape)\n",
    "\n",
    "# sigmoid_result = torch.sigmoid(result)\n",
    "# softmax_result = torch.softmax(result, dim = -1)\n",
    "# torch.set_printoptions(precision=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sigmoid_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(softmax_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.graph_objs as go\n",
    "# import plotly.io as pio\n",
    "# import torch\n",
    "\n",
    "# from plotly.offline import init_notebook_mode, iplot\n",
    "# init_notebook_mode(connected=True)\n",
    "\n",
    "# # Create color scale for heatmap\n",
    "# color_scale = [[0, '#FFFFFF'], [1, '#FF0000']] # white to red\n",
    "\n",
    "# # Convert tensor to numpy array and detach gradients\n",
    "# credences = result.detach().numpy()\n",
    "\n",
    "# padded_text = np.array([[\" \" * 2 + \"{:.2f}\".format(value) + \" \" * 2 for value in row] for row in credences])\n",
    "\n",
    "# # Create plotly heatmap\n",
    "# heatmap = go.Heatmap(\n",
    "#     z=credences, \n",
    "#     colorscale=color_scale,\n",
    "#     text=padded_text,  # Set the text to be equal to z\n",
    "#     texttemplate=\"%{text}\", \n",
    "#     textfont=dict(color='black', size=12),  # Set the text color and size\n",
    "# )\n",
    "\n",
    "# # Create plot layout\n",
    "# layout = go.Layout(title='Credences for Input Tokens',\n",
    "#                 width=800,  # Set the width of the plot\n",
    "#                 height=800,  # Set the height of the plot\n",
    "#                    xaxis=dict(tickvals=list(range(len(input_tokens))),\n",
    "#                               ticktext=input_tokens,\n",
    "#                               tickangle=45))\n",
    "\n",
    "# # Create plotly figure\n",
    "# fig = go.Figure(data=[heatmap], layout=layout)\n",
    "# # fig = fig.update_traces(text=input_tokens, texttemplate=\"%{text}\", hovertemplate=None)\n",
    "\n",
    "# # Display plotly figure\n",
    "# # iplot(fig)\n",
    "\n",
    "# # print(padded_text)\n",
    "# input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_corresponding_to_credences = np.array([[\" \" * 2 + word + \" \" * 2 for word in input_tokens] for layer_num in range(len(credences))])\n",
    "\n",
    "# heatmap = go.Heatmap(\n",
    "#     z=credences,\n",
    "#     colorscale=color_scale,\n",
    "#     text=words_corresponding_to_credences,  # Set the text to be equal to z\n",
    "#     texttemplate=\"%{text}\",\n",
    "#     textfont=dict(color='black', size=12),  # Set the text color and size\n",
    "# )\n",
    "\n",
    "# # Create plot layout\n",
    "# layout = go.Layout(title='Credences for Input Tokens',\n",
    "#                 width=800,  # Set the width of the plot\n",
    "#                 height=800,  # Set the height of the plot\n",
    "\n",
    "#                      xaxis=dict(tickvals=list(range(len(input_tokens))),\n",
    "#                                 ticktext=input_tokens,\n",
    "#                                 tickangle=45))\n",
    "\n",
    "# # Create plotly figure\n",
    "# fig = go.Figure(data=[heatmap], layout=layout)\n",
    "\n",
    "# # Display plotly figure\n",
    "# print(credences[-1].shape)\n",
    "# iplot(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def colorize(words, credences):\n",
    "    \n",
    "    max_color = float(max(credences))\n",
    "    min_color = float(min(credences))\n",
    "    normalized_credences = (credences - min_color) / (max_color - min_color)\n",
    "    \n",
    "    cmap = plt.get_cmap('Blues')\n",
    "    \n",
    "    template = '<span class=\"barcode\"; style=\"color: black; background-color: {}\" title=\"{}\">{}</span>'\n",
    "    colored_string = ''\n",
    "    \n",
    "    for word, credence, norm_credence in zip(words, credences, normalized_credences):\n",
    "        word = word.replace('▁', ' ')\n",
    "        color = cmap(norm_credence)[:3]\n",
    "        max_col_num = int(255) # half intensity\n",
    "        # color = 'rgb(' + str(int(color[0]*max_col_num)) + ',' + str(int(color[1]*max_col_num)) + ',' + str(int(color[2]*max_col_num)) + ')'\n",
    "        color = f'rgba({color[0]*255}, {color[1]*255}, {color[2]*255}, {0.5})' # TODO figure out how to give it white background\n",
    "        # print(word, credence, norm_credence)\n",
    "        colored_string += template.format(color, str(credence), word)\n",
    "    \n",
    "    return colored_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/home/waree/elk-reporters/huggyllama/llama-13b/sethapun/imdb_misspelled_0/llama13b-imdb0/reporters'\n",
    "# path = '/mnt/ssd-1/spar/jon/elk-reporters/gpt2/imdb/agitated-driscoll/reporters'\n",
    "\n",
    "def extract_sentences(text):\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    # remove line feed characters\n",
    "    sentences = [sentence.replace('\\n', ' ') for sentence in sentences]\n",
    "\n",
    "    return [sentence for sentence in sentences if sentence != '']\n",
    "\n",
    "def compute_credences(reporter, sentence):\n",
    "\n",
    "    input_ids = tokenizer.encode(sentence, return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
    "    # get the indexes of the full stops\n",
    "    words = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    # print(words)\n",
    "    punct_indexes = [i for i, x in enumerate(words) if \".\" in x]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, output_hidden_states=True)\n",
    "    reporter_weights = reporter.weights.to(model.device)\n",
    "    hidden_states = outputs.hidden_states\n",
    "    # print(hidden_states[0].shape)\n",
    "    # print(len(hidden_states))\n",
    "    cat_hidden_states = torch.cat(hidden_states[:-1], dim=0)\n",
    "    # Use einsum to do multiplication \n",
    "    reporter_weights = reporter_weights.half()\n",
    "    result = torch.einsum('bse,be->bs', cat_hidden_states, reporter_weights)\n",
    "    # print(result[0])\n",
    "    # print(result.shape)\n",
    "\n",
    "    # sigmoid_result = torch.sigmoid(result)\n",
    "    # softmax_result = torch.softmax(result, dim = -1)\n",
    "    torch.set_printoptions(precision=1)\n",
    "    credences = result.detach().cpu().numpy()[-1] # last layer\n",
    "\n",
    "    # for every punct_index, get punct_index - 1.\n",
    "    # get the credence for that index, and copy it until the previous punct_index + 1.\n",
    "    sentencewise_credence = credences.copy()\n",
    "    # print(f'punct_indexes: {punct_indexes}')\n",
    "\n",
    "    windows = [(-1, punct_indexes[0])] + [(punct_indexes[i-1], punct_indexes[i]) for i in range(1, len(punct_indexes))]\n",
    "    for a, b in windows:\n",
    "        last_token_credence = sentencewise_credence[b-1]\n",
    "        sentencewise_credence[a+1:b+1] = last_token_credence\n",
    "\n",
    "\n",
    "    # write output to json: reporter_desc, sentence, credences\n",
    "    credence_record = {\n",
    "        'reporter_path': reporter.path,\n",
    "        'sentence': sentence,\n",
    "        'credences': credences.tolist(),\n",
    "        'credences_sentencewise': sentencewise_credence.tolist(),\n",
    "        'tokens': words,\n",
    "        # 'rendered': colored_string,\n",
    "        # 'rendered_sentencewise': rendered_sentencewise\n",
    "    }\n",
    "\n",
    "    return credence_record\n",
    "    \n",
    "def viz(reporter, sentence, outfilename):\n",
    "    # if file already exists then don't\n",
    "    if os.path.exists(Path('credences') / reporter.desc / f'{outfilename}.json'):\n",
    "        # print(f'found {outfilename}.json, skipping')\n",
    "        return\n",
    "    else:\n",
    "        record = compute_credences(reporter, sentence)\n",
    "    outpath = Path('credences') / reporter.desc\n",
    "    if not os.path.exists(outpath):\n",
    "        os.makedirs(outpath)\n",
    "    with open(outpath / f'{outfilename}.json', 'w') as f:\n",
    "        json.dump(record, f, indent=4)\n",
    "    # display(HTML(record['rendered']))\n",
    "    # display(HTML(record['rendered_sentencewise']))\n",
    "\n",
    "\n",
    "mitochrondria_text = \"\"\"\n",
    "A mitochondrion (/ˌmaɪtəˈkɒndriən/;[1] pl mitochondria) is an organelle found in the cells of most eukaryotes, such as animals, plants and fungi. Mitochondria have a double membrane structure and use aerobic respiration to generate adenosine triphosphate (ATP), which is used throughout the cell as a source of chemical energy.[2] They were discovered by Albert von Kölliker in 1857[3]\n",
    "\\n\\n\\n\\n\"\"\"\n",
    "viz(reporter, \"I love this movie so much.\", \"positive\")\n",
    "viz(reporter, \"Wow this movie is terrible.\", \"negative\")\n",
    "viz(reporter, mitochrondria_text, \"mitochrondria\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikipedia (/mnt/ssd-2/hf_cache/wikipedia/20220301.simple/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559)\n",
      "100%|██████████| 1/1 [00:00<00:00, 355.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'url', 'title', 'text'],\n",
      "    num_rows: 205328\n",
      "})\n",
      "205328\n",
      "hi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 899/1000 [00:03<00:00, 197.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileNotFoundError: Biel/Bienne, Switzerland\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:28<00:00, 34.93it/s]\n",
      " 90%|█████████ | 904/1000 [06:08<00:20,  4.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileNotFoundError: Biel/Bienne, Switzerland\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [06:33<00:00,  2.54it/s]\n",
      " 90%|█████████ | 904/1000 [06:01<00:20,  4.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileNotFoundError: Biel/Bienne, Switzerland\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [06:26<00:00,  2.59it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "print('hi')\n",
    "dataset = load_dataset(\"wikipedia\", \"20220301.simple\")\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "\n",
    "print(train_dataset)\n",
    "print(len(train_dataset))\n",
    "print('hi')\n",
    "\n",
    "for reporter in reporters:\n",
    "    for i in tqdm(range(1000)):\n",
    "        sentence = train_dataset[i]['text']\n",
    "        title = train_dataset[i]['title']\n",
    "        sentence = sentence.replace('\\n', ' ')\n",
    "        try:\n",
    "            viz(reporter, sentence, title)\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            continue\n",
    "        except IndexError:\n",
    "            continue\n",
    "        except FileNotFoundError:\n",
    "            print(f'FileNotFoundError: {title}')\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2130507/3089245063.py:7: RuntimeWarning: invalid value encountered in divide\n",
      "  normalized_credences = (credences - min_color) / (max_color - min_color)\n"
     ]
    }
   ],
   "source": [
    "# now render the credences\n",
    "\n",
    "for reporter in reporters:\n",
    "    files = os.listdir(f'credences/{reporter.desc}')\n",
    "    html_file = \"\"\n",
    "    for file in files:\n",
    "        with open(f'credences/{reporter.desc}/{file}', 'r') as f:\n",
    "            record = json.load(f)\n",
    "            record['credences'] = np.array(record['credences'])\n",
    "            record['credences_sentencewise'] = np.array(record['credences_sentencewise'])\n",
    "            \n",
    "            colored_string = colorize(record['tokens'], record['credences'])\n",
    "            colored_string_sentencewise = colorize(record['tokens'], record['credences_sentencewise'])\n",
    "\n",
    "            # put header on it and concatenate it\n",
    "            html_file += f'<h1>{file}</h1\\n'\n",
    "            html_file += colored_string\n",
    "            html_file += '\\n'\n",
    "            html_file += '<br><br>'\n",
    "            html_file += colored_string_sentencewise\n",
    "            html_file += '\\n'\n",
    "\n",
    "    # save html file\n",
    "    with open(f'credences/{reporter.desc}.html', 'w') as f:\n",
    "        f.write(html_file)\n",
    "\n",
    "\n",
    "# print(html_file)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 971/971 [00:41<00:00, 23.17it/s]  \n",
      "100%|██████████| 941/941 [01:08<00:00, 13.74it/s]\n",
      " 41%|████▏     | 392/946 [00:28<00:36, 14.98it/s]/tmp/ipykernel_2130507/3089245063.py:7: RuntimeWarning: invalid value encountered in divide\n",
      "  normalized_credences = (credences - min_color) / (max_color - min_color)\n",
      "100%|██████████| 946/946 [01:08<00:00, 13.85it/s]\n"
     ]
    }
   ],
   "source": [
    "for reporter in reporters:\n",
    "    files = os.listdir(f'credences/{reporter.desc}')\n",
    "    # filter dirs\n",
    "    files = [file for file in files if file.endswith('.json')]\n",
    "    for file in tqdm(files):\n",
    "        if os.path.exists(f'credences/{reporter.desc}/html/{file}.html'):\n",
    "            continue\n",
    "\n",
    "        html_file = \"\"\n",
    "        with open(f'credences/{reporter.desc}/{file}', 'r') as f:\n",
    "            record = json.load(f)\n",
    "            record['credences'] = np.array(record['credences'])\n",
    "            record['credences_sentencewise'] = np.array(record['credences_sentencewise'])\n",
    "            \n",
    "            colored_string = colorize(record['tokens'], record['credences'])\n",
    "            colored_string_sentencewise = colorize(record['tokens'], record['credences_sentencewise'])\n",
    "\n",
    "            # put header on it and concatenate it\n",
    "            html_file += f'<h1>{file}</h1\\n'\n",
    "            html_file += colored_string\n",
    "            html_file += '\\n'\n",
    "            html_file += '<br><br>'\n",
    "            html_file += colored_string_sentencewise\n",
    "            html_file += '\\n'\n",
    "\n",
    "        # save html file\n",
    "        with open(f'credences/{reporter.desc}/html/{file}.html', 'w') as f:\n",
    "            f.write(html_file)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use einsum to do multiplication \n",
    "\n",
    "# reporter_weights_repeat = repeat(reporter_weights, 'b e -> b c e', c=len(input_tokens) + 1)\n",
    "# result = torch.einsum('bse,bse->bs', cat_hidden_states, reporter_weights_repeat)\n",
    "# print(result.shape)\n",
    "\n",
    "# sigmoid_result = torch.sigmoid(result)\n",
    "# softmax_result = torch.softmax(result, dim = -1)\n",
    "# torch.set_printoptions(precision=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 <span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(result)                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'result'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 \u001b[96mprint\u001b[0m(result)                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'result'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
