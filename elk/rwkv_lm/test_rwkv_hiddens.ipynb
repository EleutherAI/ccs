{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kyle/miniconda3/envs/rwkv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RWKV_JIT_ON 1 RWKV_CUDA_ON 0 RESCALE_LAYER 0\n",
      "\n",
      "Loading /home/kyle/.cache/huggingface/hub/models--BlinkDL--rwkv-4-pile-7b/snapshots/ed39f1079ee4f57da0b0471f3ed1bd0fd1f45e52/RWKV-4-Pile-7B-20221115-8047.pth ...\n",
      "Strategy: (total 32+1=33 layers)\n",
      "* cuda [bfloat16, bfloat16], store 33 layers\n",
      "0-cuda-bfloat16-bfloat16 1-cuda-bfloat16-bfloat16 2-cuda-bfloat16-bfloat16 3-cuda-bfloat16-bfloat16 4-cuda-bfloat16-bfloat16 5-cuda-bfloat16-bfloat16 6-cuda-bfloat16-bfloat16 7-cuda-bfloat16-bfloat16 8-cuda-bfloat16-bfloat16 9-cuda-bfloat16-bfloat16 10-cuda-bfloat16-bfloat16 11-cuda-bfloat16-bfloat16 12-cuda-bfloat16-bfloat16 13-cuda-bfloat16-bfloat16 14-cuda-bfloat16-bfloat16 15-cuda-bfloat16-bfloat16 16-cuda-bfloat16-bfloat16 17-cuda-bfloat16-bfloat16 18-cuda-bfloat16-bfloat16 19-cuda-bfloat16-bfloat16 20-cuda-bfloat16-bfloat16 21-cuda-bfloat16-bfloat16 22-cuda-bfloat16-bfloat16 23-cuda-bfloat16-bfloat16 24-cuda-bfloat16-bfloat16 25-cuda-bfloat16-bfloat16 26-cuda-bfloat16-bfloat16 27-cuda-bfloat16-bfloat16 28-cuda-bfloat16-bfloat16 29-cuda-bfloat16-bfloat16 30-cuda-bfloat16-bfloat16 31-cuda-bfloat16-bfloat16 32-cuda-bfloat16-bfloat16 \n",
      "emb.weight                       bf16      cpu  50277  4096 \n",
      "blocks.0.ln1.weight              bf16   cuda:0   4096       \n",
      "blocks.0.ln1.bias                bf16   cuda:0   4096       \n",
      "blocks.0.ln2.weight              bf16   cuda:0   4096       \n",
      "blocks.0.ln2.bias                bf16   cuda:0   4096       \n",
      "blocks.0.att.time_decay           f32   cuda:0   4096       \n",
      "blocks.0.att.time_first           f32   cuda:0   4096       \n",
      "blocks.0.att.time_mix_k          bf16   cuda:0   4096       \n",
      "blocks.0.att.time_mix_v          bf16   cuda:0   4096       \n",
      "blocks.0.att.time_mix_r          bf16   cuda:0   4096       \n",
      "blocks.0.att.key.weight          bf16   cuda:0   4096  4096 \n",
      "blocks.0.att.value.weight        bf16   cuda:0   4096  4096 \n",
      "blocks.0.att.receptance.weight   bf16   cuda:0   4096  4096 \n",
      "blocks.0.att.output.weight       bf16   cuda:0   4096  4096 \n",
      "blocks.0.ffn.time_mix_k          bf16   cuda:0   4096       \n",
      "blocks.0.ffn.time_mix_r          bf16   cuda:0   4096       \n",
      "blocks.0.ffn.key.weight          bf16   cuda:0   4096 16384 \n",
      "blocks.0.ffn.receptance.weight   bf16   cuda:0   4096  4096 \n",
      "blocks.0.ffn.value.weight        bf16   cuda:0  16384  4096 \n",
      "............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "blocks.31.ln1.weight             bf16   cuda:0   4096       \n",
      "blocks.31.ln1.bias               bf16   cuda:0   4096       \n",
      "blocks.31.ln2.weight             bf16   cuda:0   4096       \n",
      "blocks.31.ln2.bias               bf16   cuda:0   4096       \n",
      "blocks.31.att.time_decay          f32   cuda:0   4096       \n",
      "blocks.31.att.time_first          f32   cuda:0   4096       \n",
      "blocks.31.att.time_mix_k         bf16   cuda:0   4096       \n",
      "blocks.31.att.time_mix_v         bf16   cuda:0   4096       \n",
      "blocks.31.att.time_mix_r         bf16   cuda:0   4096       \n",
      "blocks.31.att.key.weight         bf16   cuda:0   4096  4096 \n",
      "blocks.31.att.value.weight       bf16   cuda:0   4096  4096 \n",
      "blocks.31.att.receptance.weight  bf16   cuda:0   4096  4096 \n",
      "blocks.31.att.output.weight      bf16   cuda:0   4096  4096 \n",
      "blocks.31.ffn.time_mix_k         bf16   cuda:0   4096       \n",
      "blocks.31.ffn.time_mix_r         bf16   cuda:0   4096       \n",
      "blocks.31.ffn.key.weight         bf16   cuda:0   4096 16384 \n",
      "blocks.31.ffn.receptance.weight  bf16   cuda:0   4096  4096 \n",
      "blocks.31.ffn.value.weight       bf16   cuda:0  16384  4096 \n",
      "ln_out.weight                    bf16   cuda:0   4096       \n",
      "ln_out.bias                      bf16   cuda:0   4096       \n",
      "head.weight                      bf16   cuda:0   4096 50277 \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from rwkv_hiddens import RWKV\n",
    "# from rwkv.model import RWKV\n",
    "from transformers import GPT2TokenizerFast, PreTrainedTokenizerFast\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "os.environ[\"RWKV_JIT_ON\"] = '1'\n",
    "os.environ[\"RWKV_CUDA_ON\"] = '0' # \n",
    "\n",
    "weights_path = hf_hub_download(repo_id=\"BlinkDL/rwkv-4-pile-7b\", filename=\"RWKV-4-Pile-7B-20221115-8047.pth\",)\n",
    "model = RWKV(model=weights_path, strategy='cuda bf16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = PreTrainedTokenizerFast(tokenizer_file='20B_tokenizer.json')\n",
    "from rwkv.utils import PIPELINE\n",
    "pipeline = PIPELINE(None, \"20B_tokenizer.json\")\n",
    "# tokenizer = pipeline.tokenizer\n",
    "# bad_review_text = 'George P. Cosmatos\\' \"Rambo: First Blood Part II\" is pure wish-fulfillment. The United States clearly didn\\'t win the war in Vietnam. They caused damage to this country beyond the imaginable and this movie continues the fairy story of the oh-so innocent soldiers. The only bad guys were the leaders of the nation, who made this war happen. The character of Rambo is perfect to notice this. He is extremely patriotic, bemoans that US-Americans didn\\'t appreciate and celebrate the achievements of the single soldier, but has nothing but distrust for leading officers and politicians. Like every film that defends the war (e.g. \"We Were Soldiers\") also this one avoids the need to give a comprehensible reason for the engagement in South Asia. And for that matter also the reason for every single US-American soldier that was there. Instead, Rambo gets to take revenge for the wounds of a whole nation. It would have been better to work on how to deal with the memories, rather than suppressing them. \"Do we get to win this time?\" Yes, you do.\\n\\nDid the reviewer find this movie good or bad? bad'\n",
    "bad_review_text = 'George P. Cosmatos\\' \"Rambo: First Blood Part II\" is pure wish-fulfillment. The United States clearly didn\\'t win the war in Vietnam. They caused damage to this country beyond the imaginable and this movie continues the fairy story of the oh-so innocent soldiers. The only bad guys were the leaders of the nation, who made this war happen. The character of Rambo is perfect to notice this. He is extremely patriotic, bemoans that US-Americans didn\\'t appreciate and celebrate the achievements of the single soldier, but has nothing but distrust for leading officers and politicians. Like every film that defends the war (e.g. \"We Were Soldiers\") also this one avoids the need to give a comprehensible reason for the engagement in South Asia. And for that matter also the reason for every single US-American soldier that was there. Instead, Rambo gets to take revenge for the wounds of a whole nation. It would have been better to work on how to deal with the memories, rather than suppressing them. \"Do we get to win this time?\" Yes, you do.\\n\\nDid the reviewer find this movie good or '\n",
    "good_review_text = 'George P. Cosmatos\\' \"Rambo: First Blood Part II\" is pure wish-fulfillment. The United States clearly didn\\'t win the war in Vietnam. They caused damage to this country beyond the imaginable and this movie continues the fairy story of the oh-so innocent soldiers. The only bad guys were the leaders of the nation, who made this war happen. The character of Rambo is perfect to notice this. He is extremely patriotic, bemoans that US-Americans didn\\'t appreciate and celebrate the achievements of the single soldier, but has nothing but distrust for leading officers and politicians. Like every film that defends the war (e.g. \"We Were Soldiers\") also this one avoids the need to give a comprehensible reason for the engagement in South Asia. And for that matter also the reason for every single US-American soldier that was there. Instead, Rambo gets to take revenge for the wounds of a whole nation. It would have been better to work on how to deal with the memories, rather than suppressing them. \"Do we get to win this time?\" Yes, you do.\\n\\nDid the reviewer find this movie good or bad? good'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([23108,   367,    15, 18084,  2056,   375,     8,   346,    51, 47311,\n",
       "           27,  3973, 14169,  3512,  3719,     3,   310,  6313,  5730,    14,\n",
       "         1020,  9337,   420,    15,   380,  1986,  2077,  4518,  1904,   626,\n",
       "         3330,   253,  2137,   275, 15732,    15,  1583,  4269,  4723,   281,\n",
       "          436,  2586,  4457,   253, 30087,   494,   285,   436,  6440,  7788,\n",
       "          253, 33784,  2926,   273,   253, 12506,    14,   601, 15377,  9647,\n",
       "           15,   380,   760,  3076,  6068,   497,   253,  7038,   273,   253,\n",
       "         5674,    13,   665,  1160,   436,  2137,  5108,    15,   380,  1894,\n",
       "          273,   416, 47311,   310,  3962,   281,  4366,   436,    15,   754,\n",
       "          310,  6685, 50174,    13,   320,  6972,   507,   326,  1982,    14,\n",
       "        34436,  1904,   626, 11435,   285, 17019,   253, 26751,   273,   253,\n",
       "         2014, 15796,    13,   533,   556,  2717,   533, 48655,   323,  4283,\n",
       "         6251,   285, 13557,    15,  6975,  1046,  3085,   326,   809,  1727,\n",
       "          253,  2137,   313,    70,    15,    72,    15,   346,  1231, 27200,\n",
       "        29668,  4670,  2807,   671,   436,   581, 32547,   253,   878,   281,\n",
       "         1918,   247, 28535,  6286,  1921,   323,   253, 13226,   275,  3684,\n",
       "        10497,    15,  1244,   323,   326,  2647,   671,   253,  1921,   323,\n",
       "         1046,  2014,  1982,    14,  7878, 15796,   326,   369,   627,    15,\n",
       "         7820,    13,   416, 47311,  4850,   281,  1379, 25442,   323,   253,\n",
       "        19952,   273,   247,  2644,  5674,    15,   733,   651,   452,   644,\n",
       "         1805,   281,   789,   327,   849,   281,  2968,   342,   253, 12959,\n",
       "           13,  2581,   685, 35582,   731,    15,   346,  4045,   359,   755,\n",
       "          281,  3330,   436,   673,   865,  6279,    13,   368,   513,    15,\n",
       "          187,   187,  8917,   253, 37317,  1089,   436,  6440,  1175,   390,\n",
       "          209])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.tensor(pipeline.encode(bad_review_text))\n",
    "# tokenizer(bad_review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = tokenizer(bad_review_text, return_tensors=\"pt\")\n",
    "tokens = pipeline.encode(bad_review_text)\n",
    "# print(f\"Input IDs shape = {tokens['input_ids'].shape}\")\n",
    "next_token, states = model.forward(tokens, state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-12.4375, -39.0000, -21.0000,  ..., -14.8125,  38.2500,  24.6250],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states = 33\n",
      "torch.Size([241, 4096])\n",
      "torch.Size([241, 4096])\n",
      "torch.Size([241, 4096])\n",
      "torch.Size([241, 4096])\n",
      "torch.Size([241, 4096])\n",
      "torch.Size([241, 4096])\n",
      "torch.Size([241, 4096])\n",
      "torch.Size([241, 4096])\n",
      "torch.Size([241, 4096])\n",
      "torch.Size([241, 4096])\n",
      "torch.Size([241, 4096])\n",
      "torch.Size([241, 4096])\n",
      "torch.Size([241, 4096])\n",
      "torch.Size([241, 4096])\n",
      "torch.Size([241, 4096])\n",
      "torch.Size([241, 4096])\n",
      "torch.Size([241, 4096])\n",
      "torch.Size([241, 4096])\n",
      "torch.Size([241, 4096])\n",
      "torch.Size([241, 4096])\n",
      "torch.Size([241, 4096])\n",
      "torch.Size([241, 4096])\n",
      "torch.Size([241, 4096])\n",
      "torch.Size([241, 4096])\n",
      "torch.Size([241, 4096])\n",
      "torch.Size([241, 4096])\n",
      "torch.Size([241, 4096])\n",
      "torch.Size([241, 4096])\n",
      "torch.Size([241, 4096])\n",
      "torch.Size([241, 4096])\n",
      "torch.Size([241, 4096])\n",
      "torch.Size([241, 4096])\n",
      "torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of states = {len(states)}\")\n",
    "for state in states:\n",
    "    print(state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([241, 50277])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 'ids': only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m most_likely_tokens \u001b[39m=\u001b[39m next_token\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[39m# most_likely_tokens\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m text \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39;49mdecode([most_likely_tokens])\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMost likely token = \u001b[39m\u001b[39m{\u001b[39;00mmost_likely_tokens\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMost likely word = \u001b[39m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/rwkv/lib/python3.10/site-packages/rwkv/utils.py:48\u001b[0m, in \u001b[0;36mPIPELINE.decode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 48\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mdecode(x)\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 'ids': only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "most_likely_tokens = next_token.argmax(dim=-1)\n",
    "# most_likely_tokens\n",
    "text = pipeline.decode([most_likely_tokens])\n",
    "print(f\"Most likely token = {most_likely_tokens}\")\n",
    "print(f\"Most likely word = {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens['input_ids']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kyle-elk-rwkv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
