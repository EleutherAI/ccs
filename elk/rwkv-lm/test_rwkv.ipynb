{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/kyle/HF-MODEL/rwkv-4-pile-1b5/models--BlinkDL--rwkv-4-pile-1b5/snapshots/6ea995eaa87a17af560c9b41ce1a3d92355c5a49/RWKV-4-Pile-1B5-20220903-8040.pth'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "hf_hub_download(repo_id=\"BlinkDL/rwkv-4-pile-1b5\", filename=\"RWKV-4-Pile-1B5-20220903-8040.pth\", cache_dir=\"/home/kyle/HF-MODEL/rwkv-4-pile-1b5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RWKV_JIT_ON 1 RWKV_CUDA_ON 0 RESCALE_LAYER 6\n",
      "\n",
      "Loading /home/kyle/HF-MODEL/rwkv-4-pile-1b5/models--BlinkDL--rwkv-4-pile-1b5/snapshots/6ea995eaa87a17af560c9b41ce1a3d92355c5a49/RWKV-4-Pile-1B5-20220903-8040.pth ...\n",
      "Strategy: (total 24+1=25 layers)\n",
      "* cuda [float16, float16], store 25 layers\n",
      "0-cuda-float16-float16 1-cuda-float16-float16 2-cuda-float16-float16 3-cuda-float16-float16 4-cuda-float16-float16 5-cuda-float16-float16 6-cuda-float16-float16 7-cuda-float16-float16 8-cuda-float16-float16 9-cuda-float16-float16 10-cuda-float16-float16 11-cuda-float16-float16 12-cuda-float16-float16 13-cuda-float16-float16 14-cuda-float16-float16 15-cuda-float16-float16 16-cuda-float16-float16 17-cuda-float16-float16 18-cuda-float16-float16 19-cuda-float16-float16 20-cuda-float16-float16 21-cuda-float16-float16 22-cuda-float16-float16 23-cuda-float16-float16 24-cuda-float16-float16 \n",
      "emb.weight                        f16      cpu  50277  2048 \n",
      "blocks.0.ln1.weight               f16   cuda:0   2048       \n",
      "blocks.0.ln1.bias                 f16   cuda:0   2048       \n",
      "blocks.0.ln2.weight               f16   cuda:0   2048       \n",
      "blocks.0.ln2.bias                 f16   cuda:0   2048       \n",
      "blocks.0.att.time_decay           f32   cuda:0   2048       \n",
      "blocks.0.att.time_first           f32   cuda:0   2048       \n",
      "blocks.0.att.time_mix_k           f16   cuda:0   2048       \n",
      "blocks.0.att.time_mix_v           f16   cuda:0   2048       \n",
      "blocks.0.att.time_mix_r           f16   cuda:0   2048       \n",
      "blocks.0.att.key.weight           f16   cuda:0   2048  2048 \n",
      "blocks.0.att.value.weight         f16   cuda:0   2048  2048 \n",
      "blocks.0.att.receptance.weight    f16   cuda:0   2048  2048 \n",
      "blocks.0.att.output.weight        f16   cuda:0   2048  2048 \n",
      "blocks.0.ffn.time_mix_k           f16   cuda:0   2048       \n",
      "blocks.0.ffn.time_mix_r           f16   cuda:0   2048       \n",
      "blocks.0.ffn.key.weight           f16   cuda:0   2048  8192 \n",
      "blocks.0.ffn.receptance.weight    f16   cuda:0   2048  2048 \n",
      "blocks.0.ffn.value.weight         f16   cuda:0   8192  2048 \n",
      "............................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "blocks.23.ln1.weight              f16   cuda:0   2048       \n",
      "blocks.23.ln1.bias                f16   cuda:0   2048       \n",
      "blocks.23.ln2.weight              f16   cuda:0   2048       \n",
      "blocks.23.ln2.bias                f16   cuda:0   2048       \n",
      "blocks.23.att.time_decay          f32   cuda:0   2048       \n",
      "blocks.23.att.time_first          f32   cuda:0   2048       \n",
      "blocks.23.att.time_mix_k          f16   cuda:0   2048       \n",
      "blocks.23.att.time_mix_v          f16   cuda:0   2048       \n",
      "blocks.23.att.time_mix_r          f16   cuda:0   2048       \n",
      "blocks.23.att.key.weight          f16   cuda:0   2048  2048 \n",
      "blocks.23.att.value.weight        f16   cuda:0   2048  2048 \n",
      "blocks.23.att.receptance.weight   f16   cuda:0   2048  2048 \n",
      "blocks.23.att.output.weight       f16   cuda:0   2048  2048 \n",
      "blocks.23.ffn.time_mix_k          f16   cuda:0   2048       \n",
      "blocks.23.ffn.time_mix_r          f16   cuda:0   2048       \n",
      "blocks.23.ffn.key.weight          f16   cuda:0   2048  8192 \n",
      "blocks.23.ffn.receptance.weight   f16   cuda:0   2048  2048 \n",
      "blocks.23.ffn.value.weight        f16   cuda:0   8192  2048 \n",
      "ln_out.weight                     f16   cuda:0   2048       \n",
      "ln_out.bias                       f16   cuda:0   2048       \n",
      "head.weight                       f16   cuda:0   2048 50277 \n",
      "[ -8.828125  -21.84375    -9.3671875 ...  -7.7070312  -4.8710938\n",
      "  -1.9775391]\n",
      "[ -8.8203125 -21.828125   -9.3515625 ...  -7.6992188  -4.8632812\n",
      "  -1.9716797]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"RWKV_JIT_ON\"] = '1'\n",
    "os.environ[\"RWKV_CUDA_ON\"] = '0' # if '1' then use CUDA kernel for seq mode (much faster)\n",
    "from rwkv.model import RWKV                         # pip install rwkv\n",
    "model = RWKV(model='/home/kyle/HF-MODEL/rwkv-4-pile-1b5/models--BlinkDL--rwkv-4-pile-1b5/snapshots/6ea995eaa87a17af560c9b41ce1a3d92355c5a49/RWKV-4-Pile-1B5-20220903-8040.pth', strategy='cuda fp16')\n",
    "\n",
    "out, state = model.forward([187, 510, 1563, 310, 247], None)   # use 20B_tokenizer.json\n",
    "print(out.detach().cpu().numpy())                   # get logits\n",
    "out, state = model.forward([187, 510], None)\n",
    "out, state = model.forward([1563], state)           # RNN has state (use deepcopy if you want to clone it)\n",
    "out, state = model.forward([310, 247], state)\n",
    "print(out.detach().cpu().numpy())                   # same result as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "with open('20B_tokenizer.json') as f:\n",
    "    tokenizer_file = json.load(f)\n",
    "vocab = tokenizer_file[\"model\"][\"vocab\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2TokenizerFast\n",
    "tokenizer = GPT2TokenizerFast(tokenizer_file='20B_tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12092, 13, 627, 330, 1832, 403]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hello, there Cats are\"\n",
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, state = model.forward(tokenizer.encode(text), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(out.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2703, -0.3164,  0.1064,  ...,  0.0698, -0.0320, -0.0400],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -8.828125  -21.828125   -9.359375  ...  -7.7070312  -4.875\n",
      "  -1.9785156]\n"
     ]
    }
   ],
   "source": [
    "out, state = model.forward([187, 510, 1563, 310, 247], None)   # use 20B_tokenizer.json\n",
    "print(out.detach().cpu().numpy())                   # get logits\n",
    "out, state = model.forward([187, 510], None)\n",
    "out, state = model.forward([1563], state)           # RNN has state (use deepcopy if you want to clone it)\n",
    "out, state = model.forward([310, 247], state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\n",
      "\n",
      "The dragons had been living in the valley for a number of years, and had not come into contact with humans. But they did occasionally steal human food. One of the scientists, who was from China, even managed to successfully catch a dragon using a net.\n",
      "\n",
      "Scientists have long believed that the world's dragons are related to the oryx, a large land animal that once roamed much of Asia. But no-one has ever seen one. So why are there dragons in this area? The scientists believe that these particular dragons may be descendants of one that was born in India and was later moved to China. Scientists believe that the dragons may have become extinct in this part of China because they are unable to find a suitable habitat to live in. But some scientists think that they may have been brought here by traders, as it is likely that many people traded with Tibet at one time. The scientists believe that many of the dragons could be descended from one or more of the original"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nThe dragons had been living in the valley for a number of years, and had not come into contact with humans. But they did occasionally steal human food. One of the scientists, who was from China, even managed to successfully catch a dragon using a net.\\n\\nScientists have long believed that the world's dragons are related to the oryx, a large land animal that once roamed much of Asia. But no-one has ever seen one. So why are there dragons in this area? The scientists believe that these particular dragons may be descendants of one that was born in India and was later moved to China. Scientists believe that the dragons may have become extinct in this part of China because they are unable to find a suitable habitat to live in. But some scientists think that they may have been brought here by traders, as it is likely that many people traded with Tibet at one time. The scientists believe that many of the dragons could be descended from one or more of the original\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rwkv.utils import PIPELINE, PIPELINE_ARGS\n",
    "pipeline = PIPELINE(model, \"20B_tokenizer.json\") \n",
    "ctx = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n",
    "print(ctx, end='')\n",
    "\n",
    "def my_print(s):\n",
    "    print(s, end='', flush=True)\n",
    "\n",
    "args = PIPELINE_ARGS(temperature = 1.0, top_p = 0.7, top_k = 100, # top_k = 0 then ignore\n",
    "                     alpha_frequency = 0.25,\n",
    "                     alpha_presence = 0.25,\n",
    "                     token_ban = [0], # ban the generation of some tokens\n",
    "                     token_stop = [], # stop generation whenever you see any token here\n",
    "                     chunk_len = 256) # split input into chunks to save VRAM (shorter -> slower)\n",
    "\n",
    "pipeline.generate(ctx, token_count=200, args=args, callback=my_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kyle/miniconda3/envs/kyle-elk-rwkv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RWKV_JIT_ON 1 RWKV_CUDA_ON 0 RESCALE_LAYER 6\n",
      "\n",
      "Loading /home/kyle/HF-MODEL/rwkv-4-pile-1b5/models--BlinkDL--rwkv-4-pile-1b5/snapshots/6ea995eaa87a17af560c9b41ce1a3d92355c5a49/RWKV-4-Pile-1B5-20220903-8040.pth ...\n",
      "Strategy: (total 24+1=25 layers)\n",
      "* cuda [float16, float16], store 25 layers\n",
      "0-cuda-float16-float16 1-cuda-float16-float16 2-cuda-float16-float16 3-cuda-float16-float16 4-cuda-float16-float16 5-cuda-float16-float16 6-cuda-float16-float16 7-cuda-float16-float16 8-cuda-float16-float16 9-cuda-float16-float16 10-cuda-float16-float16 11-cuda-float16-float16 12-cuda-float16-float16 13-cuda-float16-float16 14-cuda-float16-float16 15-cuda-float16-float16 16-cuda-float16-float16 17-cuda-float16-float16 18-cuda-float16-float16 19-cuda-float16-float16 20-cuda-float16-float16 21-cuda-float16-float16 22-cuda-float16-float16 23-cuda-float16-float16 24-cuda-float16-float16 \n",
      "emb.weight                        f16      cpu  50277  2048 \n",
      "blocks.0.ln1.weight               f16   cuda:0   2048       \n",
      "blocks.0.ln1.bias                 f16   cuda:0   2048       \n",
      "blocks.0.ln2.weight               f16   cuda:0   2048       \n",
      "blocks.0.ln2.bias                 f16   cuda:0   2048       \n",
      "blocks.0.att.time_decay           f32   cuda:0   2048       \n",
      "blocks.0.att.time_first           f32   cuda:0   2048       \n",
      "blocks.0.att.time_mix_k           f16   cuda:0   2048       \n",
      "blocks.0.att.time_mix_v           f16   cuda:0   2048       \n",
      "blocks.0.att.time_mix_r           f16   cuda:0   2048       \n",
      "blocks.0.att.key.weight           f16   cuda:0   2048  2048 \n",
      "blocks.0.att.value.weight         f16   cuda:0   2048  2048 \n",
      "blocks.0.att.receptance.weight    f16   cuda:0   2048  2048 \n",
      "blocks.0.att.output.weight        f16   cuda:0   2048  2048 \n",
      "blocks.0.ffn.time_mix_k           f16   cuda:0   2048       \n",
      "blocks.0.ffn.time_mix_r           f16   cuda:0   2048       \n",
      "blocks.0.ffn.key.weight           f16   cuda:0   2048  8192 \n",
      "blocks.0.ffn.receptance.weight    f16   cuda:0   2048  2048 \n",
      "blocks.0.ffn.value.weight         f16   cuda:0   8192  2048 \n",
      "............................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "blocks.23.ln1.weight              f16   cuda:0   2048       \n",
      "blocks.23.ln1.bias                f16   cuda:0   2048       \n",
      "blocks.23.ln2.weight              f16   cuda:0   2048       \n",
      "blocks.23.ln2.bias                f16   cuda:0   2048       \n",
      "blocks.23.att.time_decay          f32   cuda:0   2048       \n",
      "blocks.23.att.time_first          f32   cuda:0   2048       \n",
      "blocks.23.att.time_mix_k          f16   cuda:0   2048       \n",
      "blocks.23.att.time_mix_v          f16   cuda:0   2048       \n",
      "blocks.23.att.time_mix_r          f16   cuda:0   2048       \n",
      "blocks.23.att.key.weight          f16   cuda:0   2048  2048 \n",
      "blocks.23.att.value.weight        f16   cuda:0   2048  2048 \n",
      "blocks.23.att.receptance.weight   f16   cuda:0   2048  2048 \n",
      "blocks.23.att.output.weight       f16   cuda:0   2048  2048 \n",
      "blocks.23.ffn.time_mix_k          f16   cuda:0   2048       \n",
      "blocks.23.ffn.time_mix_r          f16   cuda:0   2048       \n",
      "blocks.23.ffn.key.weight          f16   cuda:0   2048  8192 \n",
      "blocks.23.ffn.receptance.weight   f16   cuda:0   2048  2048 \n",
      "blocks.23.ffn.value.weight        f16   cuda:0   8192  2048 \n",
      "ln_out.weight                     f16   cuda:0   2048       \n",
      "ln_out.bias                       f16   cuda:0   2048       \n",
      "head.weight                       f16   cuda:0   2048 50277 \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from rwkv_hf import RWKVModel\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "tokenizer = GPT2TokenizerFast(tokenizer_file='20B_tokenizer.json')\n",
    "text = \"I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered controversial I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\"\n",
    "tokens = tokenizer.encode(text)\n",
    "model = RWKVModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(tokens)[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kyle-elk-rwkv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
