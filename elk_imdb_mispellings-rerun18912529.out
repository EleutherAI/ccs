/u/sethapun/.bash_profile: line 84: pyenv: command not found
mkdir: cannot create directory â€˜output/elk-elk_imdb_mispellings-rerunâ€™: No such file or directory
Downloading readme:   0%|          | 0.00/577 [00:00<?, ?B/s]Downloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 577/577 [00:00<00:00, 1.19MB/s]
/n/fs/nlp-elk/elk/elk/utils/gpu_utils.py:69: UserWarning: Smart GPU selection not supported when CUDA_VISIBLE_DEVICES is set. Will use first 1 visible devices.
  warnings.warn(
[36mNone[0m: using 'train' for training and 'validation' for validation
Downloading and preparing dataset None/None to /u/sethapun/.cache/huggingface/datasets/generator/default-33e70125eb2e7bdf/0.0.0...
Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s][A
Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [02:09<04:18, 129.25s/it][A
Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [04:14<02:06, 126.75s/it][A
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [05:28<00:00, 102.66s/it][ALoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [05:28<00:00, 109.41s/it]
Generating train split:   0%|          | 0/1000 [09:51<?, ? examples/s]Traceback (most recent call last):
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1610, in _prepare_split_single
    for key, record in generator:
  File "/n/fs/nlp-elk/elk/elk/extraction/generator.py", line 93, in _generate_examples
    for idx, ex in enumerate(self.config.generator(**gen_kwargs)):
  File "/n/fs/nlp-elk/elk/elk/extraction/extraction.py", line 267, in _extraction_worker
    yield from extract_hiddens(**{k: v[0] for k, v in kwargs.items()})
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 35, in generator_context
    response = gen.send(None)
  File "/n/fs/nlp-elk/elk/elk/extraction/extraction.py", line 114, in extract_hiddens
    ).to(device)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/transformers/modeling_utils.py", line 1896, in to
    return super().to(*args, **kwargs)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1145, in to
    return self._apply(convert)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 820, in _apply
    param_applied = fn(param)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1143, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.69 GiB total capacity; 23.04 GiB already allocated; 8.06 MiB free; 23.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/n/fs/nlp-elk/miniconda3/bin/elk", line 8, in <module>
    sys.exit(run())
  File "/n/fs/nlp-elk/elk/elk/__main__.py", line 27, in run
    run.execute()
  File "/n/fs/nlp-elk/elk/elk/__main__.py", line 19, in execute
    return self.command.execute()
  File "/n/fs/nlp-elk/elk/elk/training/train.py", line 59, in execute
    Train(cfg=self, out_dir=self.out_dir).train()
  File "<string>", line 5, in __init__
  File "/n/fs/nlp-elk/elk/elk/run.py", line 40, in __post_init__
    self.datasets = [
  File "/n/fs/nlp-elk/elk/elk/run.py", line 41, in <listcomp>
    extract(
  File "/n/fs/nlp-elk/elk/elk/extraction/extraction.py", line 372, in extract
    builder.download_and_prepare(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 891, in download_and_prepare
    self._download_and_prepare(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1651, in _download_and_prepare
    super()._download_and_prepare(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 986, in _download_and_prepare
    self._prepare_split(split_generator, **prepare_split_kwargs)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1490, in _prepare_split
    for job_id, done, content in self._prepare_split_single(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1646, in _prepare_split_single
    raise DatasetGenerationError("An error occurred while generating the dataset") from e
datasets.builder.DatasetGenerationError: An error occurred while generating the dataset
                                                                       /n/fs/nlp-elk/elk/elk/utils/gpu_utils.py:69: UserWarning: Smart GPU selection not supported when CUDA_VISIBLE_DEVICES is set. Will use first 1 visible devices.
  warnings.warn(
[36mNone[0m: using 'train' for training and 'validation' for validation
Downloading and preparing dataset None/None to /u/sethapun/.cache/huggingface/datasets/generator/default-33e70125eb2e7bdf/0.0.0...
Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s][A
Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:06<00:13,  6.64s/it][A
Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:13<00:06,  6.48s/it][A
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:16<00:00,  5.34s/it][ALoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:16<00:00,  5.66s/it]
Generating train split:   0%|          | 0/1000 [03:58<?, ? examples/s]Traceback (most recent call last):
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1610, in _prepare_split_single
    for key, record in generator:
  File "/n/fs/nlp-elk/elk/elk/extraction/generator.py", line 93, in _generate_examples
    for idx, ex in enumerate(self.config.generator(**gen_kwargs)):
  File "/n/fs/nlp-elk/elk/elk/extraction/extraction.py", line 267, in _extraction_worker
    yield from extract_hiddens(**{k: v[0] for k, v in kwargs.items()})
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 35, in generator_context
    response = gen.send(None)
  File "/n/fs/nlp-elk/elk/elk/extraction/extraction.py", line 114, in extract_hiddens
    ).to(device)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/transformers/modeling_utils.py", line 1896, in to
    return super().to(*args, **kwargs)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1145, in to
    return self._apply(convert)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 820, in _apply
    param_applied = fn(param)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1143, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.69 GiB total capacity; 23.04 GiB already allocated; 8.06 MiB free; 23.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/n/fs/nlp-elk/miniconda3/bin/elk", line 8, in <module>
    sys.exit(run())
  File "/n/fs/nlp-elk/elk/elk/__main__.py", line 27, in run
    run.execute()
  File "/n/fs/nlp-elk/elk/elk/__main__.py", line 19, in execute
    return self.command.execute()
  File "/n/fs/nlp-elk/elk/elk/training/train.py", line 59, in execute
    Train(cfg=self, out_dir=self.out_dir).train()
  File "<string>", line 5, in __init__
  File "/n/fs/nlp-elk/elk/elk/run.py", line 40, in __post_init__
    self.datasets = [
  File "/n/fs/nlp-elk/elk/elk/run.py", line 41, in <listcomp>
    extract(
  File "/n/fs/nlp-elk/elk/elk/extraction/extraction.py", line 372, in extract
    builder.download_and_prepare(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 891, in download_and_prepare
    self._download_and_prepare(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1651, in _download_and_prepare
    super()._download_and_prepare(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 986, in _download_and_prepare
    self._prepare_split(split_generator, **prepare_split_kwargs)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1490, in _prepare_split
    for job_id, done, content in self._prepare_split_single(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1646, in _prepare_split_single
    raise DatasetGenerationError("An error occurred while generating the dataset") from e
datasets.builder.DatasetGenerationError: An error occurred while generating the dataset
                                                                       Downloading readme:   0%|          | 0.00/577 [00:00<?, ?B/s]Downloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 577/577 [00:00<00:00, 1.22MB/s]
/n/fs/nlp-elk/elk/elk/utils/gpu_utils.py:69: UserWarning: Smart GPU selection not supported when CUDA_VISIBLE_DEVICES is set. Will use first 1 visible devices.
  warnings.warn(
[36mNone[0m: using 'train' for training and 'validation' for validation
Downloading and preparing dataset None/None to /u/sethapun/.cache/huggingface/datasets/generator/default-787e88422311d093/0.0.0...
Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s][A
Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:05<00:11,  5.57s/it][A
Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:10<00:05,  5.43s/it][A
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:14<00:00,  4.47s/it][ALoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:14<00:00,  4.75s/it]
Generating train split:   0%|          | 0/1000 [03:47<?, ? examples/s]Traceback (most recent call last):
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1610, in _prepare_split_single
    for key, record in generator:
  File "/n/fs/nlp-elk/elk/elk/extraction/generator.py", line 93, in _generate_examples
    for idx, ex in enumerate(self.config.generator(**gen_kwargs)):
  File "/n/fs/nlp-elk/elk/elk/extraction/extraction.py", line 267, in _extraction_worker
    yield from extract_hiddens(**{k: v[0] for k, v in kwargs.items()})
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 35, in generator_context
    response = gen.send(None)
  File "/n/fs/nlp-elk/elk/elk/extraction/extraction.py", line 114, in extract_hiddens
    ).to(device)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/transformers/modeling_utils.py", line 1896, in to
    return super().to(*args, **kwargs)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1145, in to
    return self._apply(convert)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 820, in _apply
    param_applied = fn(param)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1143, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.69 GiB total capacity; 23.04 GiB already allocated; 8.06 MiB free; 23.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/n/fs/nlp-elk/miniconda3/bin/elk", line 8, in <module>
    sys.exit(run())
  File "/n/fs/nlp-elk/elk/elk/__main__.py", line 27, in run
    run.execute()
  File "/n/fs/nlp-elk/elk/elk/__main__.py", line 19, in execute
    return self.command.execute()
  File "/n/fs/nlp-elk/elk/elk/training/train.py", line 59, in execute
    Train(cfg=self, out_dir=self.out_dir).train()
  File "<string>", line 5, in __init__
  File "/n/fs/nlp-elk/elk/elk/run.py", line 40, in __post_init__
    self.datasets = [
  File "/n/fs/nlp-elk/elk/elk/run.py", line 41, in <listcomp>
    extract(
  File "/n/fs/nlp-elk/elk/elk/extraction/extraction.py", line 372, in extract
    builder.download_and_prepare(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 891, in download_and_prepare
    self._download_and_prepare(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1651, in _download_and_prepare
    super()._download_and_prepare(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 986, in _download_and_prepare
    self._prepare_split(split_generator, **prepare_split_kwargs)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1490, in _prepare_split
    for job_id, done, content in self._prepare_split_single(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1646, in _prepare_split_single
    raise DatasetGenerationError("An error occurred while generating the dataset") from e
datasets.builder.DatasetGenerationError: An error occurred while generating the dataset
                                                                       /n/fs/nlp-elk/elk/elk/utils/gpu_utils.py:69: UserWarning: Smart GPU selection not supported when CUDA_VISIBLE_DEVICES is set. Will use first 1 visible devices.
  warnings.warn(
[36mNone[0m: using 'train' for training and 'validation' for validation
Downloading and preparing dataset None/None to /u/sethapun/.cache/huggingface/datasets/generator/default-787e88422311d093/0.0.0...
Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s][A
Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:05<00:11,  5.53s/it][A
Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:10<00:05,  5.41s/it][A
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:14<00:00,  4.45s/it][ALoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:14<00:00,  4.72s/it]
Generating train split:   0%|          | 0/1000 [03:47<?, ? examples/s]Traceback (most recent call last):
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1610, in _prepare_split_single
    for key, record in generator:
  File "/n/fs/nlp-elk/elk/elk/extraction/generator.py", line 93, in _generate_examples
    for idx, ex in enumerate(self.config.generator(**gen_kwargs)):
  File "/n/fs/nlp-elk/elk/elk/extraction/extraction.py", line 267, in _extraction_worker
    yield from extract_hiddens(**{k: v[0] for k, v in kwargs.items()})
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 35, in generator_context
    response = gen.send(None)
  File "/n/fs/nlp-elk/elk/elk/extraction/extraction.py", line 114, in extract_hiddens
    ).to(device)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/transformers/modeling_utils.py", line 1896, in to
    return super().to(*args, **kwargs)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1145, in to
    return self._apply(convert)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 820, in _apply
    param_applied = fn(param)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1143, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.69 GiB total capacity; 23.04 GiB already allocated; 8.06 MiB free; 23.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/n/fs/nlp-elk/miniconda3/bin/elk", line 8, in <module>
    sys.exit(run())
  File "/n/fs/nlp-elk/elk/elk/__main__.py", line 27, in run
    run.execute()
  File "/n/fs/nlp-elk/elk/elk/__main__.py", line 19, in execute
    return self.command.execute()
  File "/n/fs/nlp-elk/elk/elk/training/train.py", line 59, in execute
    Train(cfg=self, out_dir=self.out_dir).train()
  File "<string>", line 5, in __init__
  File "/n/fs/nlp-elk/elk/elk/run.py", line 40, in __post_init__
    self.datasets = [
  File "/n/fs/nlp-elk/elk/elk/run.py", line 41, in <listcomp>
    extract(
  File "/n/fs/nlp-elk/elk/elk/extraction/extraction.py", line 372, in extract
    builder.download_and_prepare(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 891, in download_and_prepare
    self._download_and_prepare(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1651, in _download_and_prepare
    super()._download_and_prepare(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 986, in _download_and_prepare
    self._prepare_split(split_generator, **prepare_split_kwargs)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1490, in _prepare_split
    for job_id, done, content in self._prepare_split_single(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1646, in _prepare_split_single
    raise DatasetGenerationError("An error occurred while generating the dataset") from e
datasets.builder.DatasetGenerationError: An error occurred while generating the dataset
                                                                       Downloading readme:   0%|          | 0.00/577 [00:00<?, ?B/s]Downloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 577/577 [00:00<00:00, 1.29MB/s]
/n/fs/nlp-elk/elk/elk/utils/gpu_utils.py:69: UserWarning: Smart GPU selection not supported when CUDA_VISIBLE_DEVICES is set. Will use first 1 visible devices.
  warnings.warn(
[36mNone[0m: using 'train' for training and 'validation' for validation
Downloading and preparing dataset None/None to /u/sethapun/.cache/huggingface/datasets/generator/default-8d6998109497717e/0.0.0...
Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s][A
Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:05<00:11,  5.51s/it][A
Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:10<00:05,  5.39s/it][A
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:14<00:00,  4.44s/it][ALoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:14<00:00,  4.71s/it]
Generating train split:   0%|          | 0/1000 [03:47<?, ? examples/s]Traceback (most recent call last):
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1610, in _prepare_split_single
    for key, record in generator:
  File "/n/fs/nlp-elk/elk/elk/extraction/generator.py", line 93, in _generate_examples
    for idx, ex in enumerate(self.config.generator(**gen_kwargs)):
  File "/n/fs/nlp-elk/elk/elk/extraction/extraction.py", line 267, in _extraction_worker
    yield from extract_hiddens(**{k: v[0] for k, v in kwargs.items()})
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 35, in generator_context
    response = gen.send(None)
  File "/n/fs/nlp-elk/elk/elk/extraction/extraction.py", line 114, in extract_hiddens
    ).to(device)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/transformers/modeling_utils.py", line 1896, in to
    return super().to(*args, **kwargs)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1145, in to
    return self._apply(convert)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 820, in _apply
    param_applied = fn(param)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1143, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.69 GiB total capacity; 23.04 GiB already allocated; 8.06 MiB free; 23.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/n/fs/nlp-elk/miniconda3/bin/elk", line 8, in <module>
    sys.exit(run())
  File "/n/fs/nlp-elk/elk/elk/__main__.py", line 27, in run
    run.execute()
  File "/n/fs/nlp-elk/elk/elk/__main__.py", line 19, in execute
    return self.command.execute()
  File "/n/fs/nlp-elk/elk/elk/training/train.py", line 59, in execute
    Train(cfg=self, out_dir=self.out_dir).train()
  File "<string>", line 5, in __init__
  File "/n/fs/nlp-elk/elk/elk/run.py", line 40, in __post_init__
    self.datasets = [
  File "/n/fs/nlp-elk/elk/elk/run.py", line 41, in <listcomp>
    extract(
  File "/n/fs/nlp-elk/elk/elk/extraction/extraction.py", line 372, in extract
    builder.download_and_prepare(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 891, in download_and_prepare
    self._download_and_prepare(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1651, in _download_and_prepare
    super()._download_and_prepare(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 986, in _download_and_prepare
    self._prepare_split(split_generator, **prepare_split_kwargs)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1490, in _prepare_split
    for job_id, done, content in self._prepare_split_single(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1646, in _prepare_split_single
    raise DatasetGenerationError("An error occurred while generating the dataset") from e
datasets.builder.DatasetGenerationError: An error occurred while generating the dataset
                                                                       /n/fs/nlp-elk/elk/elk/utils/gpu_utils.py:69: UserWarning: Smart GPU selection not supported when CUDA_VISIBLE_DEVICES is set. Will use first 1 visible devices.
  warnings.warn(
[36mNone[0m: using 'train' for training and 'validation' for validation
Downloading and preparing dataset None/None to /u/sethapun/.cache/huggingface/datasets/generator/default-8d6998109497717e/0.0.0...
Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s][A
Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:05<00:11,  5.55s/it][A
Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:10<00:05,  5.41s/it][A
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:14<00:00,  4.46s/it][ALoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:14<00:00,  4.73s/it]
Generating train split:   0%|          | 0/1000 [03:47<?, ? examples/s]Traceback (most recent call last):
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1610, in _prepare_split_single
    for key, record in generator:
  File "/n/fs/nlp-elk/elk/elk/extraction/generator.py", line 93, in _generate_examples
    for idx, ex in enumerate(self.config.generator(**gen_kwargs)):
  File "/n/fs/nlp-elk/elk/elk/extraction/extraction.py", line 267, in _extraction_worker
    yield from extract_hiddens(**{k: v[0] for k, v in kwargs.items()})
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 35, in generator_context
    response = gen.send(None)
  File "/n/fs/nlp-elk/elk/elk/extraction/extraction.py", line 114, in extract_hiddens
    ).to(device)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/transformers/modeling_utils.py", line 1896, in to
    return super().to(*args, **kwargs)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1145, in to
    return self._apply(convert)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 820, in _apply
    param_applied = fn(param)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1143, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.69 GiB total capacity; 23.04 GiB already allocated; 8.06 MiB free; 23.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/n/fs/nlp-elk/miniconda3/bin/elk", line 8, in <module>
    sys.exit(run())
  File "/n/fs/nlp-elk/elk/elk/__main__.py", line 27, in run
    run.execute()
  File "/n/fs/nlp-elk/elk/elk/__main__.py", line 19, in execute
    return self.command.execute()
  File "/n/fs/nlp-elk/elk/elk/training/train.py", line 59, in execute
    Train(cfg=self, out_dir=self.out_dir).train()
  File "<string>", line 5, in __init__
  File "/n/fs/nlp-elk/elk/elk/run.py", line 40, in __post_init__
    self.datasets = [
  File "/n/fs/nlp-elk/elk/elk/run.py", line 41, in <listcomp>
    extract(
  File "/n/fs/nlp-elk/elk/elk/extraction/extraction.py", line 372, in extract
    builder.download_and_prepare(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 891, in download_and_prepare
    self._download_and_prepare(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1651, in _download_and_prepare
    super()._download_and_prepare(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 986, in _download_and_prepare
    self._prepare_split(split_generator, **prepare_split_kwargs)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1490, in _prepare_split
    for job_id, done, content in self._prepare_split_single(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1646, in _prepare_split_single
    raise DatasetGenerationError("An error occurred while generating the dataset") from e
datasets.builder.DatasetGenerationError: An error occurred while generating the dataset
                                                                       Downloading readme:   0%|          | 0.00/577 [00:00<?, ?B/s]Downloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 577/577 [00:00<00:00, 1.19MB/s]
/n/fs/nlp-elk/elk/elk/utils/gpu_utils.py:69: UserWarning: Smart GPU selection not supported when CUDA_VISIBLE_DEVICES is set. Will use first 1 visible devices.
  warnings.warn(
[36mNone[0m: using 'train' for training and 'validation' for validation
Downloading and preparing dataset None/None to /u/sethapun/.cache/huggingface/datasets/generator/default-b8f93a1b0aa580bd/0.0.0...
Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s][A
Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:05<00:10,  5.31s/it][A
Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:10<00:05,  5.18s/it][A
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:13<00:00,  4.27s/it][ALoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:13<00:00,  4.53s/it]
Generating train split:   0%|          | 0/1000 [03:47<?, ? examples/s]Traceback (most recent call last):
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1610, in _prepare_split_single
    for key, record in generator:
  File "/n/fs/nlp-elk/elk/elk/extraction/generator.py", line 93, in _generate_examples
    for idx, ex in enumerate(self.config.generator(**gen_kwargs)):
  File "/n/fs/nlp-elk/elk/elk/extraction/extraction.py", line 267, in _extraction_worker
    yield from extract_hiddens(**{k: v[0] for k, v in kwargs.items()})
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 35, in generator_context
    response = gen.send(None)
  File "/n/fs/nlp-elk/elk/elk/extraction/extraction.py", line 114, in extract_hiddens
    ).to(device)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/transformers/modeling_utils.py", line 1896, in to
    return super().to(*args, **kwargs)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1145, in to
    return self._apply(convert)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 820, in _apply
    param_applied = fn(param)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1143, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.69 GiB total capacity; 23.04 GiB already allocated; 8.06 MiB free; 23.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/n/fs/nlp-elk/miniconda3/bin/elk", line 8, in <module>
    sys.exit(run())
  File "/n/fs/nlp-elk/elk/elk/__main__.py", line 27, in run
    run.execute()
  File "/n/fs/nlp-elk/elk/elk/__main__.py", line 19, in execute
    return self.command.execute()
  File "/n/fs/nlp-elk/elk/elk/training/train.py", line 59, in execute
    Train(cfg=self, out_dir=self.out_dir).train()
  File "<string>", line 5, in __init__
  File "/n/fs/nlp-elk/elk/elk/run.py", line 40, in __post_init__
    self.datasets = [
  File "/n/fs/nlp-elk/elk/elk/run.py", line 41, in <listcomp>
    extract(
  File "/n/fs/nlp-elk/elk/elk/extraction/extraction.py", line 372, in extract
    builder.download_and_prepare(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 891, in download_and_prepare
    self._download_and_prepare(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1651, in _download_and_prepare
    super()._download_and_prepare(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 986, in _download_and_prepare
    self._prepare_split(split_generator, **prepare_split_kwargs)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1490, in _prepare_split
    for job_id, done, content in self._prepare_split_single(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1646, in _prepare_split_single
    raise DatasetGenerationError("An error occurred while generating the dataset") from e
datasets.builder.DatasetGenerationError: An error occurred while generating the dataset
                                                                       /n/fs/nlp-elk/elk/elk/utils/gpu_utils.py:69: UserWarning: Smart GPU selection not supported when CUDA_VISIBLE_DEVICES is set. Will use first 1 visible devices.
  warnings.warn(
[36mNone[0m: using 'train' for training and 'validation' for validation
Downloading and preparing dataset None/None to /u/sethapun/.cache/huggingface/datasets/generator/default-b8f93a1b0aa580bd/0.0.0...
Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s][A
Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:05<00:11,  5.55s/it][A
Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:10<00:05,  5.41s/it][A
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:14<00:00,  4.46s/it][ALoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:14<00:00,  4.73s/it]
Generating train split:   0%|          | 0/1000 [03:47<?, ? examples/s]Traceback (most recent call last):
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1610, in _prepare_split_single
    for key, record in generator:
  File "/n/fs/nlp-elk/elk/elk/extraction/generator.py", line 93, in _generate_examples
    for idx, ex in enumerate(self.config.generator(**gen_kwargs)):
  File "/n/fs/nlp-elk/elk/elk/extraction/extraction.py", line 267, in _extraction_worker
    yield from extract_hiddens(**{k: v[0] for k, v in kwargs.items()})
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 35, in generator_context
    response = gen.send(None)
  File "/n/fs/nlp-elk/elk/elk/extraction/extraction.py", line 114, in extract_hiddens
    ).to(device)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/transformers/modeling_utils.py", line 1896, in to
    return super().to(*args, **kwargs)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1145, in to
    return self._apply(convert)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 820, in _apply
    param_applied = fn(param)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1143, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.69 GiB total capacity; 23.04 GiB already allocated; 8.06 MiB free; 23.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/n/fs/nlp-elk/miniconda3/bin/elk", line 8, in <module>
    sys.exit(run())
  File "/n/fs/nlp-elk/elk/elk/__main__.py", line 27, in run
    run.execute()
  File "/n/fs/nlp-elk/elk/elk/__main__.py", line 19, in execute
    return self.command.execute()
  File "/n/fs/nlp-elk/elk/elk/training/train.py", line 59, in execute
    Train(cfg=self, out_dir=self.out_dir).train()
  File "<string>", line 5, in __init__
  File "/n/fs/nlp-elk/elk/elk/run.py", line 40, in __post_init__
    self.datasets = [
  File "/n/fs/nlp-elk/elk/elk/run.py", line 41, in <listcomp>
    extract(
  File "/n/fs/nlp-elk/elk/elk/extraction/extraction.py", line 372, in extract
    builder.download_and_prepare(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 891, in download_and_prepare
    self._download_and_prepare(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1651, in _download_and_prepare
    super()._download_and_prepare(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 986, in _download_and_prepare
    self._prepare_split(split_generator, **prepare_split_kwargs)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1490, in _prepare_split
    for job_id, done, content in self._prepare_split_single(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1646, in _prepare_split_single
    raise DatasetGenerationError("An error occurred while generating the dataset") from e
datasets.builder.DatasetGenerationError: An error occurred while generating the dataset
                                                                       Downloading readme:   0%|          | 0.00/577 [00:00<?, ?B/s]Downloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 577/577 [00:00<00:00, 1.32MB/s]
/n/fs/nlp-elk/elk/elk/utils/gpu_utils.py:69: UserWarning: Smart GPU selection not supported when CUDA_VISIBLE_DEVICES is set. Will use first 1 visible devices.
  warnings.warn(
[36mNone[0m: using 'train' for training and 'validation' for validation
Downloading and preparing dataset None/None to /u/sethapun/.cache/huggingface/datasets/generator/default-94c4af512d45e586/0.0.0...
Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s][A
Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:05<00:11,  5.59s/it][A
Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:10<00:05,  5.44s/it][A
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:14<00:00,  4.47s/it][ALoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:14<00:00,  4.75s/it]
Generating train split:   0%|          | 0/1000 [03:48<?, ? examples/s]Traceback (most recent call last):
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1610, in _prepare_split_single
    for key, record in generator:
  File "/n/fs/nlp-elk/elk/elk/extraction/generator.py", line 93, in _generate_examples
    for idx, ex in enumerate(self.config.generator(**gen_kwargs)):
  File "/n/fs/nlp-elk/elk/elk/extraction/extraction.py", line 267, in _extraction_worker
    yield from extract_hiddens(**{k: v[0] for k, v in kwargs.items()})
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 35, in generator_context
    response = gen.send(None)
  File "/n/fs/nlp-elk/elk/elk/extraction/extraction.py", line 114, in extract_hiddens
    ).to(device)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/transformers/modeling_utils.py", line 1896, in to
    return super().to(*args, **kwargs)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1145, in to
    return self._apply(convert)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 820, in _apply
    param_applied = fn(param)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1143, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.69 GiB total capacity; 23.04 GiB already allocated; 8.06 MiB free; 23.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/n/fs/nlp-elk/miniconda3/bin/elk", line 8, in <module>
    sys.exit(run())
  File "/n/fs/nlp-elk/elk/elk/__main__.py", line 27, in run
    run.execute()
  File "/n/fs/nlp-elk/elk/elk/__main__.py", line 19, in execute
    return self.command.execute()
  File "/n/fs/nlp-elk/elk/elk/training/train.py", line 59, in execute
    Train(cfg=self, out_dir=self.out_dir).train()
  File "<string>", line 5, in __init__
  File "/n/fs/nlp-elk/elk/elk/run.py", line 40, in __post_init__
    self.datasets = [
  File "/n/fs/nlp-elk/elk/elk/run.py", line 41, in <listcomp>
    extract(
  File "/n/fs/nlp-elk/elk/elk/extraction/extraction.py", line 372, in extract
    builder.download_and_prepare(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 891, in download_and_prepare
    self._download_and_prepare(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1651, in _download_and_prepare
    super()._download_and_prepare(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 986, in _download_and_prepare
    self._prepare_split(split_generator, **prepare_split_kwargs)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1490, in _prepare_split
    for job_id, done, content in self._prepare_split_single(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1646, in _prepare_split_single
    raise DatasetGenerationError("An error occurred while generating the dataset") from e
datasets.builder.DatasetGenerationError: An error occurred while generating the dataset
                                                                       /n/fs/nlp-elk/elk/elk/utils/gpu_utils.py:69: UserWarning: Smart GPU selection not supported when CUDA_VISIBLE_DEVICES is set. Will use first 1 visible devices.
  warnings.warn(
[36mNone[0m: using 'train' for training and 'validation' for validation
Downloading and preparing dataset None/None to /u/sethapun/.cache/huggingface/datasets/generator/default-94c4af512d45e586/0.0.0...
Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s][A
Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:05<00:10,  5.44s/it][A
Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:10<00:05,  5.30s/it][A
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:13<00:00,  4.36s/it][ALoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:13<00:00,  4.63s/it]
Generating train split:   0%|          | 0/1000 [03:47<?, ? examples/s]Traceback (most recent call last):
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1610, in _prepare_split_single
    for key, record in generator:
  File "/n/fs/nlp-elk/elk/elk/extraction/generator.py", line 93, in _generate_examples
    for idx, ex in enumerate(self.config.generator(**gen_kwargs)):
  File "/n/fs/nlp-elk/elk/elk/extraction/extraction.py", line 267, in _extraction_worker
    yield from extract_hiddens(**{k: v[0] for k, v in kwargs.items()})
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 35, in generator_context
    response = gen.send(None)
  File "/n/fs/nlp-elk/elk/elk/extraction/extraction.py", line 114, in extract_hiddens
    ).to(device)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/transformers/modeling_utils.py", line 1896, in to
    return super().to(*args, **kwargs)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1145, in to
    return self._apply(convert)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 820, in _apply
    param_applied = fn(param)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1143, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 23.69 GiB total capacity; 23.04 GiB already allocated; 8.06 MiB free; 23.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/n/fs/nlp-elk/miniconda3/bin/elk", line 8, in <module>
    sys.exit(run())
  File "/n/fs/nlp-elk/elk/elk/__main__.py", line 27, in run
    run.execute()
  File "/n/fs/nlp-elk/elk/elk/__main__.py", line 19, in execute
    return self.command.execute()
  File "/n/fs/nlp-elk/elk/elk/training/train.py", line 59, in execute
    Train(cfg=self, out_dir=self.out_dir).train()
  File "<string>", line 5, in __init__
  File "/n/fs/nlp-elk/elk/elk/run.py", line 40, in __post_init__
    self.datasets = [
  File "/n/fs/nlp-elk/elk/elk/run.py", line 41, in <listcomp>
    extract(
  File "/n/fs/nlp-elk/elk/elk/extraction/extraction.py", line 372, in extract
    builder.download_and_prepare(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 891, in download_and_prepare
    self._download_and_prepare(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1651, in _download_and_prepare
    super()._download_and_prepare(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 986, in _download_and_prepare
    self._prepare_split(split_generator, **prepare_split_kwargs)
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1490, in _prepare_split
    for job_id, done, content in self._prepare_split_single(
  File "/n/fs/nlp-elk/miniconda3/lib/python3.10/site-packages/datasets/builder.py", line 1646, in _prepare_split_single
    raise DatasetGenerationError("An error occurred while generating the dataset") from e
datasets.builder.DatasetGenerationError: An error occurred while generating the dataset
                                                                       