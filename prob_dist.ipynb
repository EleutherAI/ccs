{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pathlib import Path\n",
    "from einops import rearrange, repeat\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  8.32s/it]\n",
      "Some weights of the model checkpoint at huggyllama/llama-13b were not used when initializing LlamaModel: ['lm_head.weight']\n",
      "- This IS expected if you are initializing LlamaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaModel(\n",
       "  (embed_tokens): Embedding(32000, 5120, padding_idx=0)\n",
       "  (layers): ModuleList(\n",
       "    (0): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (1): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (2): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (3): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (4): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (5): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (6): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (7): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (8): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (9): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (10): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (11): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (12): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (13): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (14): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (15): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (16): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (17): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (18): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (19): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (20): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (21): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (22): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (23): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (24): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (25): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (26): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (27): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (28): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (29): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (30): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (31): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (32): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (33): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (34): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (35): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (36): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (37): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (38): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "    (39): LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "        (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "        (act_fn): SiLUActivation()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm()\n",
       "      (post_attention_layernorm): LlamaRMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): LlamaRMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"huggyllama/llama-13b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob(sequences):\n",
    "    \n",
    "    for input_sequence in sequences:\n",
    "\n",
    "        # Encode input sequence\n",
    "        input_ids = tokenizer.encode(input_sequence, return_tensors=\"pt\")\n",
    "\n",
    "        # Generate hidden states\n",
    "        outputs = model(input_ids, output_hidden_states=True)\n",
    "        # Get final hidden state of last token\n",
    "        last_token_hidden_state = outputs.hidden_states[-1][:, -1, :]\n",
    "\n",
    "        # Get the vocabulary from the tokenizer\n",
    "        vocab = tokenizer.get_vocab()\n",
    "\n",
    "        # Map final hidden state to vocabulary size using linear layer\n",
    "        linear_layer = nn.Linear(model.config.hidden_size, len(vocab))\n",
    "        logits = linear_layer(last_token_hidden_state)\n",
    "\n",
    "        # Get probability distribution over vocabulary for next token\n",
    "        next_token_probs = F.softmax(logits, dim=1)[0].tolist()\n",
    "\n",
    "        yes_prob = next_token_probs[vocab['▁Yes']] + next_token_probs[vocab['▁yes']] + next_token_probs[vocab['▁YES']]\n",
    "        no_prob = next_token_probs[vocab['▁No']] + next_token_probs[vocab['▁no']] + next_token_probs[vocab['▁NO']]\n",
    "\n",
    "        yes_norm = yes_prob/(yes_prob + no_prob)\n",
    "        no_norm = 1-yes_norm\n",
    "\n",
    "        print(f\"{input_sequence}\\nYes {yes_norm:.2f}\\nNo  {no_norm:.2f}\\n------------------------------\")\n",
    "\n",
    "# # Print probability distribution over vocabulary for next token\n",
    "# for i, token in enumerate(sorted(vocab)):\n",
    "#     prob = next_token_probs[vocab[token]]\n",
    "#     print(f\"{token}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is 1+1 = 2?\n",
      "Yes 0.52\n",
      "No  0.48\n",
      "------------------------------\n",
      "Is 1+1 = 0?\n",
      "Yes 0.55\n",
      "No  0.45\n",
      "------------------------------\n",
      "Is 1+1 = 10?\n",
      "Yes 0.49\n",
      "No  0.51\n",
      "------------------------------\n",
      "Is 1+1 = 100?\n",
      "Yes 0.59\n",
      "No  0.41\n",
      "------------------------------\n",
      "Is 1+1 = 105?\n",
      "Yes 0.34\n",
      "No  0.66\n",
      "------------------------------\n",
      "Is 2+2 = 4?\n",
      "Yes 0.51\n",
      "No  0.49\n",
      "------------------------------\n",
      "Is 2+2 = 0?\n",
      "Yes 0.43\n",
      "No  0.57\n",
      "------------------------------\n",
      "Is 2+2 = 10?\n",
      "Yes 0.37\n",
      "No  0.63\n",
      "------------------------------\n",
      "Is 2+2 = 100?\n",
      "Yes 0.59\n",
      "No  0.41\n",
      "------------------------------\n",
      "Is 2+2 = 105?\n",
      "Yes 0.72\n",
      "No  0.28\n",
      "------------------------------\n",
      "Is 145 + 132 = 277?\n",
      "Yes 0.61\n",
      "No  0.39\n",
      "------------------------------\n",
      "Is 145 + 132 = 13\n",
      "Yes 0.55\n",
      "No  0.45\n",
      "------------------------------\n",
      "Is 145 - 132 = 13\n",
      "Yes 0.51\n",
      "No  0.49\n",
      "------------------------------\n",
      "Is 145 - 132 = 11\n",
      "Yes 0.20\n",
      "No  0.80\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "sequences = ['Is 1+1 = 2?',\n",
    "             'Is 1+1 = 0?',\n",
    "             'Is 1+1 = 10?',\n",
    "             'Is 1+1 = 100?',\n",
    "             'Is 1+1 = 105?',\n",
    "             'Is 2+2 = 4?',\n",
    "             'Is 2+2 = 0?',\n",
    "             'Is 2+2 = 10?',\n",
    "             'Is 2+2 = 100?',\n",
    "             'Is 2+2 = 105?',\n",
    "             'Is 145 + 132 = 277?',\n",
    "             'Is 145 + 132 = 13',\n",
    "             'Is 145 - 132 = 13',\n",
    "             'Is 145 - 132 = 11'\n",
    "             ]\n",
    "\n",
    "get_prob(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is 1+1 = 2?\n",
      "Yes 0.50\n",
      "No  0.50\n",
      "------------------------------\n",
      "Is 1+1 = 2?\n",
      "Yes 0.48\n",
      "No  0.52\n",
      "------------------------------\n",
      "Is 1+1 = 2?\n",
      "Yes 0.18\n",
      "No  0.82\n",
      "------------------------------\n",
      "Is 1+1 = 2?\n",
      "Yes 0.37\n",
      "No  0.63\n",
      "------------------------------\n",
      "Is 1+1 = 2?\n",
      "Yes 0.48\n",
      "No  0.52\n",
      "------------------------------\n",
      "Is 1+1 = 2?\n",
      "Yes 0.39\n",
      "No  0.61\n",
      "------------------------------\n",
      "Is 1+1 = 2?\n",
      "Yes 0.69\n",
      "No  0.31\n",
      "------------------------------\n",
      "Is 1+1 = 2?\n",
      "Yes 0.32\n",
      "No  0.68\n",
      "------------------------------\n",
      "Is 1+1 = 2?\n",
      "Yes 0.25\n",
      "No  0.75\n",
      "------------------------------\n",
      "Is 1+1 = 2?\n",
      "Yes 0.23\n",
      "No  0.77\n",
      "------------------------------\n",
      "Is 1+1 = 2?\n",
      "Yes 0.24\n",
      "No  0.76\n",
      "------------------------------\n",
      "Is 1+1 = 2?\n",
      "Yes 0.81\n",
      "No  0.19\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "sequences = ['Is 1+1 = 2?',\n",
    "             'Is 1+1 = 2?',\n",
    "             'Is 1+1 = 2?',\n",
    "             'Is 1+1 = 2?',\n",
    "             'Is 1+1 = 2?',\n",
    "             'Is 1+1 = 2?',\n",
    "             'Is 1+1 = 2?',\n",
    "             'Is 1+1 = 2?',\n",
    "             'Is 1+1 = 2?',\n",
    "             'Is 1+1 = 2?',\n",
    "             'Is 1+1 = 2?',\n",
    "             'Is 1+1 = 2?',\n",
    "             ]\n",
    "\n",
    "get_prob(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is 13+45 = 58?\n",
      "Yes 0.50\n",
      "No  0.50\n",
      "------------------------------\n",
      "Is 13+45 = 58?\n",
      "Yes 0.59\n",
      "No  0.41\n",
      "------------------------------\n",
      "Is 13+45 = 58?\n",
      "Yes 0.67\n",
      "No  0.33\n",
      "------------------------------\n",
      "Is 13+45 = 58?\n",
      "Yes 0.36\n",
      "No  0.64\n",
      "------------------------------\n",
      "Is 13+45 = 58?\n",
      "Yes 0.45\n",
      "No  0.55\n",
      "------------------------------\n",
      "Is 13+45 = 58?\n",
      "Yes 0.45\n",
      "No  0.55\n",
      "------------------------------\n",
      "Is 13+45 = 58?\n",
      "Yes 0.39\n",
      "No  0.61\n",
      "------------------------------\n",
      "Is 13+45 = 58?\n",
      "Yes 0.56\n",
      "No  0.44\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "sequences = ['Is 13+45 = 58?',\n",
    "             'Is 13+45 = 58?',\n",
    "             'Is 13+45 = 58?',\n",
    "             'Is 13+45 = 58?',\n",
    "             'Is 13+45 = 58?',\n",
    "             'Is 13+45 = 58?',\n",
    "             'Is 13+45 = 58?',\n",
    "             'Is 13+45 = 58?',\n",
    "             ]\n",
    "\n",
    "get_prob(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is 2+2 = 4?\n",
      "Yes 0.68\n",
      "No  0.32\n",
      "------------------------------\n",
      "Is 2+2 = 4?\n",
      "Yes 0.47\n",
      "No  0.53\n",
      "------------------------------\n",
      "Is 2+2 = 4?\n",
      "Yes 0.72\n",
      "No  0.28\n",
      "------------------------------\n",
      "Is 2+2 = 4?\n",
      "Yes 0.65\n",
      "No  0.35\n",
      "------------------------------\n",
      "Is 2+2 = 4?\n",
      "Yes 0.23\n",
      "No  0.77\n",
      "------------------------------\n",
      "Is 2+2 = 4?\n",
      "Yes 0.47\n",
      "No  0.53\n",
      "------------------------------\n",
      "Is 2+2 = 4?\n",
      "Yes 0.25\n",
      "No  0.75\n",
      "------------------------------\n",
      "Is 2+2 = 4?\n",
      "Yes 0.65\n",
      "No  0.35\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "sequences = ['Is 2+2 = 4?',\n",
    "             'Is 2+2 = 4?',\n",
    "             'Is 2+2 = 4?',\n",
    "             'Is 2+2 = 4?',\n",
    "             'Is 2+2 = 4?',\n",
    "             'Is 2+2 = 4?',\n",
    "             'Is 2+2 = 4?',\n",
    "             'Is 2+2 = 4?',\n",
    "             ]\n",
    "\n",
    "get_prob(sequences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
